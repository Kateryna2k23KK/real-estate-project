<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analysis - Real Estate Price Prediction</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <!-- Insert horizontal image as header -->
        <img src="assets/rs1.JPEG" alt="Real Estate Analysis" style="width: 100%; height: auto;">
    </header>

    <main>
        <!-- Task and Goal -->
        <section>
            <h2><strong>Task:</strong></h2>
            <p>The task of this project is to analyze a real estate dataset, predict house prices, and evaluate the performance of different machine learning models.</p>
            
            <h2><strong>Goal:</strong></h2>
            <p>The goal is to apply machine learning techniques, perform feature engineering, and evaluate the models using metrics like RMSE and R².</p>
        </section>

        <!-- Step 2: Data Analysis -->
        <section>
            <h2><strong>#2 Initial Data Analysis</strong></h2>
            <p>In this section, we begin with an initial exploration of the dataset, handle missing values, and prepare the data for machine learning models.</p>
            
            <h3>1. Data Overview</h3>
            <pre>
                # Example code for loading the dataset and viewing the first rows
                import pandas as pd
                data = pd.read_csv('train.csv')
                print(data.head())
            </pre>
            <p>We load the dataset and display the first few rows to understand its structure and check for any missing values.</p>

            <h3>2. Handling Missing Values</h3>
            <pre>
                # Fill missing values with the median for numerical columns
                data['LotFrontage'] = data['LotFrontage'].fillna(data['LotFrontage'].median())
                data['MasVnrType'] = data['MasVnrType'].fillna(data['MasVnrType'].mode()[0])
                # Further preprocessing steps
            </pre>
            <p>Missing values are handled using the median for numerical features and the mode for categorical features. This ensures the data is ready for machine learning models.</p>

            <h3>3. Feature Engineering</h3>
            <p>Categorical variables are transformed using One-Hot Encoding to make them suitable for model training:</p>
            <pre>
                # Apply One-Hot Encoding
                data = pd.get_dummies(data, drop_first=True)
            </pre>
            <p>Now the data is prepared for machine learning algorithms, with categorical variables properly encoded.</p>
        </section>

        <!-- Step 3: Model Training and Results -->
        <section>
            <h2><strong>#3 Model Training and Results</strong></h2>
            <p>After preprocessing, we train three different models to predict house prices: Linear Regression, Random Forest, and XGBoost. Below are the performance results:</p>

            <h3>1. Model Performance</h3>
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>RMSE</th>
                        <th>R²</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Linear Regression</td>
                        <td>XXX</td>
                        <td>YYY</td>
                    </tr>
                    <tr>
                        <td>Random Forest</td>
                        <td>XXX</td>
                        <td>YYY</td>
                    </tr>
                    <tr>
                        <td>XGBoost</td>
                        <td>XXX</td>
                        <td>YYY</td>
                    </tr>
                </tbody>
            </table>
            
            <p>The best performing model is XGBoost, which provides the lowest RMSE and highest R², indicating its superior predictive ability.</p>

            <!-- Visualizing results -->
            <h3>2. Visualizations</h3>
            <p>Below are visualizations comparing actual vs predicted house prices and the distribution of residuals for the XGBoost model:</p>

            <h4>1. Actual vs Predicted Prices (XGBoost)</h4>
            <img src="assets/actual_vs_predicted.png" alt="Actual vs Predicted Prices" class="result-image">

            <h4>2. Residuals Distribution</h4>
            <img src="assets/residuals_distribution.png" alt="Residuals Distribution" class="result-image">
        </section>

        <!-- Step 4: Feature Importance -->
        <section>
            <h2><strong>#4 Feature Importance</strong></h2>
            <p>Using the XGBoost model, we analyze the importance of different features using SHAP (SHapley Additive exPlanations) values:</p>
            <img src="assets/feature_importance.png" alt="Feature Importance" class="result-image">
            <p>The most important features influencing house prices are:</p>
            <ul>
                <li><strong>OverallQual:</strong> Overall quality of the house.</li>
                <li><strong>GrLivArea:</strong> Above grade (ground) living area in square feet.</li>
                <li><strong>GarageCars:</strong> Number of cars the garage can hold.</li>
            </ul>
        </section>

        <!-- Step 5: Hyperparameter Tuning -->
        <section>
            <h2><strong>#5 Model Tuning</strong></h2>
            <p>We use GridSearchCV to tune the hyperparameters of Random Forest and XGBoost models. The best hyperparameters are as follows:</p>
            <ul>
                <li><strong>Best Random Forest Parameters:</strong> {'n_estimators': 200, 'max_depth': 20, ...}</li>
                <li><strong>Best XGBoost Parameters:</strong> {'learning_rate': 0.1, 'n_estimators': 100, ...}</li>
            </ul>
            <p>These hyperparameters lead to better model performance and higher generalization capabilities.</p>
        </section>

        <!-- Final Thoughts and Example -->
        <section>
            <h2><strong>#6 Conclusion and Example Usage</strong></h2>
            <p>In this analysis, we successfully applied machine learning models to predict house prices. The XGBoost model performed the best. Below is an example of how to use the trained model for predictions:</p>
            <pre>
                # Example code for making predictions
                import xgboost as xgb
                model = xgb.XGBRegressor()
                model.fit(X_train, y_train)
                predictions = model.predict(X_test)
                print(predictions)
            </pre>
            <p>This code demonstrates how to train and make predictions using the XGBoost model. After training, predictions can be made on new data.</p>
        </section>

        <!-- Code Folder Link -->
        <section>
            <h2>7. Code</h2>
            <p>The full source code for this analysis, including data preprocessing, model training, and evaluation, is available in the <a href="code/">Code</a> folder.</p>
        </section>
    </main>

    <footer>
        <p>Created by Your Name</p>
    </footer>
</body>
</html>
